{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">! Important: These are the file paths of the various datasets. The data files are already in the zip file. If you decide to adjust the file structure, you must ensure you change the file paths in the first cell (under heading “File paths”) to correspond with the correct files. If you do not, the Jupyter notebook will not be able to read the data. !</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = \"./original_data/car-evaluation/car.data\";\n",
    "abalone_data = \"./original_data/abalone/abalone.data\";\n",
    "madelon_train_data = \"./original_data/madelon/madelon_train.data\";\n",
    "madelon_train_label_data = \"./original_data/madelon/madelon_train.labels\";\n",
    "madelon_valid_data = \"./original_data/madelon/madelon_valid.data\";\n",
    "madelon_valid_label_data = \"./original_data/madelon/madelon_valid.labels\";\n",
    "madelon_test_data = \"./original_data/madelon/madelon_test.data\";\n",
    "kdd_data = \"./original_data/kddcup99/kddcup.data_10_percent\";\n",
    "kdd_test_data = \"./original_data/kddcup99/kddcup.testdata.unlabeled_10_percent\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installs/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"CSVFiles\")\n",
    "Pkg.add(\"ScikitLearn\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"PyCall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSVFiles\n",
    "using DataFrames\n",
    "using ScikitLearn\n",
    "using StatsBase\n",
    "using PyCall\n",
    "joblib = pyimport(\"joblib\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Module model_selection has been ported to Julia - try `import ScikitLearn: CrossValidation` instead\n",
      "└ @ ScikitLearn.Skcore /Users/ambergfisher/.julia/packages/ScikitLearn/NJwUf/src/Skcore.jl:179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.multiclass.OneVsRestClassifier'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-learn imports\n",
    "@sk_import preprocessing: (OrdinalEncoder, LabelEncoder, MinMaxScaler)\n",
    "@sk_import model_selection: (cross_val_score, GridSearchCV, RandomizedSearchCV)\n",
    "@sk_import ensemble: (RandomForestClassifier, BaggingClassifier)\n",
    "@sk_import svm: LinearSVC\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import naive_bayes: GaussianNB\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import multiclass: OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training & testing (70-30 split). \n",
    "The method takes 70% of the values between 1 and the size of the features set to get the training indices and\n",
    "uses that to get the training features and labels. The testing indices are whatever is left over (and the training features and labels follow from the indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_test_split (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_test_split(features, labels)\n",
    "    n = size(features, 1)\n",
    "    train_ind = sort(StatsBase.sample(1:n, floor(Int, 0.7 * n), replace=false))\n",
    "    test_ind = setdiff(1:n, train_ind)\n",
    "\n",
    "    train_features = features[train_ind, :]\n",
    "    test_features = features[test_ind, :]\n",
    "    \n",
    "    train_labels = labels[train_ind]\n",
    "    test_labels = labels[test_ind]\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate the data as desired. The mapper is required in Julia to map column names to something that Python can handle. The modifier array contains the a list of column keys and the modifier class to use on them. The data type is only used to determine if the data should be converted into integers (used for the OrdinalEncoding and LabelEncoding classes). At the end, a DataFrame of the modified data is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform_features (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transform_features(raw_data, modifier_arr, dataType)\n",
    "    mapper = DataFrameMapper(modifier_arr)\n",
    "    if dataType == \"Int\"\n",
    "        data = floor.(Int, fit_transform!(mapper, raw_data))\n",
    "    else\n",
    "        data = fit_transform!(mapper, raw_data)\n",
    "    end\n",
    "    return DataFrame(data, propertynames(raw_data))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the model based on the average of the 5-fold cross validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cv_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function average_cv_score(model, features, labels, crossval=5)\n",
    "    scores = cross_val_score(model, features, labels, cv=crossval)\n",
    "    return mean(scores)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and split into features and labels\n",
    "car_df = DataFrame(load(File(format\"CSV\", car_data); header_exists=false, colnames=[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"value\"]));\n",
    "car_features, car_labels = car_df[:, 1:end-1], car_df[:, end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# Ordinal encode, then normalize the data. Finally, split the data into testing and training sets.\n",
    "car_features = transform_features(car_features, [(propertynames(car_features), OrdinalEncoder())], \"Int\")\n",
    "car_features = transform_features(car_features, [(propertynames(car_features), MinMaxScaler())], nothing)\n",
    "car_train_features, car_train_labels, car_test_features, car_test_labels = train_test_split(car_features, car_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Arrays because Julia is annoying\n",
    "car_train_features = convert(Array, car_train_features)\n",
    "car_train_labels = convert(Array, car_train_labels)\n",
    "car_test_features = convert(Array, car_test_features)\n",
    "car_test_labels = convert(Array, car_test_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into binary \"acceptable\" vs \"unacceptable\" \n",
    "# (acc & unacc are already labeled properly so we just need to change vgood and good to acc)\n",
    "temp_train = copy(car_train_labels)\n",
    "temp_test = copy(car_test_labels)\n",
    "car_train_labels[temp_train.==\"vgood\"] .= \"acc\"\n",
    "car_train_labels[temp_train.==\"good\"] .= \"acc\"\n",
    "car_test_labels[temp_test.==\"vgood\"] .= \"acc\"\n",
    "car_test_labels[temp_test.==\"good\"] .= \"acc\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and split into features and labels\n",
    "abalone_df = DataFrame(load(File(format\"CSV\", abalone_data); header_exists=false, colnames=[\"sex\", \"length\", \"diameter\", \"height\", \"whole_weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"rings\"]));\n",
    "abalone_features, abalone_labels = abalone_df[:, 1:end - 1], abalone_df[:, end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# Label encode, then normalize the data. Finally, split the data into testing and training sets.\n",
    "abalone_features.sex = LabelEncoder().fit_transform(abalone_features[:, :sex])\n",
    "abalone_features = transform_features(abalone_features, [(propertynames(abalone_features), MinMaxScaler())], nothing)\n",
    "abalone_train_features, abalone_train_labels, abalone_test_features, abalone_test_labels = train_test_split(abalone_features, abalone_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Arrays because Julia is annoying\n",
    "abalone_train_features = convert(Array{Any}, abalone_train_features)\n",
    "abalone_train_labels = convert(Array{Any}, abalone_train_labels)\n",
    "abalone_test_features = convert(Array{Any}, abalone_test_features)\n",
    "abalone_test_labels = convert(Array{Any}, abalone_test_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into binary \"young\" vs \"old\"\n",
    "temp_train = copy(abalone_train_labels)\n",
    "temp_test = copy(abalone_test_labels)\n",
    "abalone_train_labels[temp_train.<=9] .= \"young\"\n",
    "abalone_train_labels[temp_train.>9] .= \"old\"\n",
    "abalone_test_labels[temp_test.<=9] .= \"young\"\n",
    "abalone_test_labels[temp_test.>9] .= \"old\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Madelon Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and combine the training and validation sets into a single training set\n",
    "madelon_train_features = DataFrame(load(File(format\"CSV\", madelon_train_data); spacedelim=true, header_exists=false));\n",
    "madelon_train_labels = DataFrame(load(File(format\"CSV\", madelon_train_label_data); header_exists=false, colnames=[\"Value\"]));\n",
    "madelon_valid_features = DataFrame(load(File(format\"CSV\", madelon_valid_data); spacedelim=true, header_exists=false));\n",
    "madelon_valid_labels = DataFrame(load(File(format\"CSV\", madelon_valid_label_data); header_exists=false, colnames=[\"Value\"]));\n",
    "\n",
    "madelon_train_features = vcat(madelon_train_features, madelon_valid_features)\n",
    "madelon_train_labels = vcat(madelon_train_labels, madelon_valid_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the testing features\n",
    "madelon_test_features = DataFrame(load(File(format\"CSV\", madelon_test_data); spacedelim=true, header_exists=false));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Arrays because Julia is annoying\n",
    "madelon_train_features = convert(Array, madelon_train_features)\n",
    "madelon_train_labels = convert(Array, madelon_train_labels)\n",
    "madelon_test_features = convert(Array, madelon_test_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD Cup 1999 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data and add column names. Then split into training features and labels\n",
    "kdd_df = DataFrame(load(File(format\"CSV\", kdd_data); header_exists=false));\n",
    "rename!(kdd_df, [:duration, :protocol_type, :service, :flag, :src_bytes, :dst_bytes, :land, :wrong_fragment, :urgent, :hot, :num_failed_logins, :logged_in, :num_compromised, :root_shell, :su_attempted, :num_root, :num_file_creations, :num_shells, :num_access_files, :num_outbound_cmds, :is_host_login, :is_guest_login, :count, :srv_count, :serror_rate, :srv_serror_rate, :rerror_rate, :srv_rerror_rate, :same_srv_rate, :diff_srv_rate, :srv_diff_host_rate, :dst_host_count, :dst_host_srv_count, :dst_host_same_srv_rate, :dst_host_diff_srv_rate, :dst_host_same_src_port_rate, :dst_host_srv_diff_host_rate, :dst_host_serror_rate, :dst_host_srv_serror_rate, :dst_host_rerror_rate, :dst_host_srv_rerror_rate, :value]);\n",
    "kdd_train_features, kdd_train_labels = kdd_df[:, 1:end - 1], kdd_df[:, end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the testing features and add column names\n",
    "kdd_test_features = DataFrame(load(File(format\"CSV\", kdd_test_data); header_exists=false))\n",
    "rename!(kdd_test_features, [:duration, :protocol_type, :service, :flag, :src_bytes, :dst_bytes, :land, :wrong_fragment, :urgent, :hot, :num_failed_logins, :logged_in, :num_compromised, :root_shell, :su_attempted, :num_root, :num_file_creations, :num_shells, :num_access_files, :num_outbound_cmds, :is_host_login, :is_guest_login, :count, :srv_count, :serror_rate, :srv_serror_rate, :rerror_rate, :srv_rerror_rate, :same_srv_rate, :diff_srv_rate, :srv_diff_host_rate, :dst_host_count, :dst_host_srv_count, :dst_host_same_srv_rate, :dst_host_diff_srv_rate, :dst_host_same_src_port_rate, :dst_host_srv_diff_host_rate, :dst_host_serror_rate, :dst_host_srv_serror_rate, :dst_host_rerror_rate, :dst_host_srv_rerror_rate]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# Ordinal encode all columns with type String and reassign to kdd_train_features with updated column types\n",
    "string_cols = findall(x -> x == String, eltype.(eachcol(kdd_train_features)))\n",
    "kdd_train_strings = kdd_train_features[:, string_cols]\n",
    "kdd_train_features[!, string_cols] = convert.(Int64, transform_features(kdd_train_strings, [(propertynames(kdd_train_strings), OrdinalEncoder())], \"Int\"))\n",
    "kdd_test_strings = kdd_test_features[:, string_cols]\n",
    "kdd_test_features[!, string_cols] = convert.(Int64, transform_features(kdd_test_strings, [(propertynames(kdd_test_strings), OrdinalEncoder())], \"Int\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all columns\n",
    "kdd_train_features = transform_features(kdd_train_features, [(propertynames(kdd_train_features), MinMaxScaler())], nothing)\n",
    "kdd_test_features = transform_features(kdd_test_features, [(propertynames(kdd_test_features), MinMaxScaler())], nothing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Arrays because Julia is annoying\n",
    "kdd_train_features = convert(Array, kdd_train_features)\n",
    "kdd_train_labels = convert(Array, kdd_train_labels)\n",
    "kdd_test_features = convert(Array, kdd_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into binary \"normal.\" vs \"abnormal.\"\n",
    "kdd_train_labels[kdd_train_labels.!=\"normal.\"] .= \"abnormal.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees (Scikit-Learn Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.804594 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Dict{Any,Any}(\"max_features\" => nothing,\"max_depth\" => nothing,\"min_samples_leaf\" => 2,\"bootstrap\" => false,\"min_samples_split\" => 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:880: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/ambergfisher/.julia/conda/3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:593: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "sys:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "# Number of features to consider at every split\n",
    "max_features = [\"sqrt\", 0.5, 0.8, 0.9, nothing]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [1,5,10,15,nothing]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [true, false]\n",
    "\n",
    "# Put into a dictionary\n",
    "random_grid = Dict([\"max_features\" => max_features, \"max_depth\" => max_depth, \"min_samples_split\" => min_samples_split, \"min_samples_leaf\" => min_samples_leaf, \"bootstrap\" => bootstrap])\n",
    "\n",
    "# Create a model and create the randomized search class\n",
    "car_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "car_forest_rand = RandomizedSearchCV(estimator = car_forest_model, param_distributions = random_grid, n_iter = 25, cv = 3, random_state=42, n_jobs = -1)\n",
    "# Timing how long it takes to do the actual search\n",
    "@time car_forest_rand.fit(car_train_features, car_train_labels)\n",
    "println(car_forest_rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8487534721031513\n",
      "Training time:\n",
      "  0.120501 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Testing time:\n",
      "  0.015986 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters into a dictionary with Symbol keys\n",
    "# n_jobs just speeds up the actual fitting process later\n",
    "best_params = Dict(car_forest_rand.best_params_)\n",
    "best_params[\"n_jobs\"] = -1\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in best_params)\n",
    "\n",
    "# Score the model\n",
    "car_forest_scores = average_cv_score(RandomForestClassifier(;best_params...), car_train_features, car_train_labels)\n",
    "println(car_forest_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time car_forest_model.fit(car_train_features, car_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time car_forest_model.predict(car_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.640548 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Dict{Any,Any}(\"max_features\" => \"sqrt\",\"max_depth\" => 15,\"min_samples_leaf\" => 2,\"bootstrap\" => true,\"min_samples_split\" => 10)"
     ]
    }
   ],
   "source": [
    "# Number of features to consider at every split\n",
    "max_features = [\"sqrt\", 0.5, 0.8, 0.9, nothing]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [1,5,10,15,nothing]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [true, false]\n",
    "\n",
    "# Put into a dictionary\n",
    "random_grid = Dict([\"max_features\" => max_features, \"max_depth\" => max_depth, \"min_samples_split\" => min_samples_split, \"min_samples_leaf\" => min_samples_leaf, \"bootstrap\" => bootstrap])\n",
    "\n",
    "# Create a model and create the randomized search class\n",
    "abalone_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "abalone_forest_rand = RandomizedSearchCV(estimator = abalone_forest_model, param_distributions = random_grid, n_iter = 25, cv = 3, random_state=42, n_jobs = -1)\n",
    "# Timing how long it takes to do the actual search\n",
    "@time abalone_forest_rand.fit(abalone_train_features, abalone_train_labels)\n",
    "print(abalone_forest_rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7960988174686804\n",
      "Training time:\n",
      "  0.143955 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Testing time:\n",
      "  0.023975 seconds (22.59 k allocations: 353.250 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters into a dictionary with Symbol keys\n",
    "# n_jobs just speeds up the actual fitting process later\n",
    "best_params = Dict(abalone_forest_rand.best_params_)\n",
    "best_params[\"n_jobs\"] = -1\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in best_params)\n",
    "\n",
    "# Score the model\n",
    "abalone_forest_scores = average_cv_score(RandomForestClassifier(;best_params...), abalone_train_features, abalone_train_labels)\n",
    "println(abalone_forest_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time abalone_forest_model.fit(abalone_train_features, abalone_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time abalone_forest_model.predict(abalone_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98.219515 seconds (17 allocations: 976 bytes)\n",
      "Dict{Any,Any}(\"max_features\" => 0.5,\"max_depth\" => nothing,\"min_samples_leaf\" => 1,\"bootstrap\" => false,\"min_samples_split\" => 5)"
     ]
    }
   ],
   "source": [
    "# Number of features to consider at every split\n",
    "max_features = [\"sqrt\", 0.5, 0.8, 0.9, nothing]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [1,5,10,15,nothing]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [true, false]\n",
    "\n",
    "# Put into a dictionary\n",
    "random_grid = Dict([\"max_features\" => max_features, \"max_depth\" => max_depth, \"min_samples_split\" => min_samples_split, \"min_samples_leaf\" => min_samples_leaf, \"bootstrap\" => bootstrap])\n",
    "\n",
    "# Create a model and create the randomized search class\n",
    "madelon_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "madelon_forest_rand = RandomizedSearchCV(estimator = madelon_forest_model, param_distributions = random_grid, n_iter = 10, cv = 3, random_state=42, n_jobs = -1)\n",
    "# Timing how long it takes to do the actual search\n",
    "@time madelon_forest_rand.fit(madelon_train_features, madelon_train_labels)\n",
    "print(madelon_forest_rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8496153846153846\n",
      "Training time:\n",
      "  0.468943 seconds (17 allocations: 976 bytes)\n",
      "Testing time:\n",
      "  0.021089 seconds (33 allocations: 15.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters into a dictionary with Symbol keys\n",
    "# n_jobs just speeds up the actual fitting process later\n",
    "best_params = Dict(madelon_forest_rand.best_params_)\n",
    "best_params[\"n_jobs\"] = -1\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in best_params)\n",
    "\n",
    "# Score the model\n",
    "madelon_forest_scores = average_cv_score(RandomForestClassifier(;best_params...), madelon_train_features, madelon_train_labels)\n",
    "println(madelon_forest_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time madelon_forest_model.fit(madelon_train_features, madelon_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time madelon_forest_model.predict(madelon_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "kdd_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Score the model\n",
    "kdd_forest_scores = average_cv_score(kdd_forest_model, kdd_train_features, kdd_train_labels)\n",
    "println(kdd_forest_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time kdd_forest_model.fit(kdd_train_features, kdd_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time kdd_forest_model.predict(kdd_test_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (LinearSVC with Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6890127224717946\n",
      "Training time:\n",
      "  0.034530 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Testing time:\n",
      "  0.006674 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "car_svc_model = BaggingClassifier(LinearSVC(), n_jobs=-1)\n",
    "\n",
    "# Score the model\n",
    "car_svc_scores = average_cv_score(car_svc_model, car_train_features, car_train_labels)\n",
    "println(car_svc_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time car_svc_model.fit(car_train_features, car_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time car_svc_model.predict(car_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7786541388596184\n",
      "Training time:\n",
      "  0.217020 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Testing time:\n",
      "  0.038418 seconds (22.59 k allocations: 353.250 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "abalone_svc_model = BaggingClassifier(LinearSVC(), n_jobs=-1)\n",
    "\n",
    "# Score the model\n",
    "abalone_svc_scores = average_cv_score(abalone_svc_model, abalone_train_features, abalone_train_labels)\n",
    "println(abalone_svc_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time abalone_svc_model.fit(abalone_train_features, abalone_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time abalone_svc_model.predict(abalone_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5496153846153845\n",
      "Training time:\n",
      "  5.118226 seconds (17 allocations: 976 bytes)\n",
      "Testing time:\n",
      "  0.182036 seconds (33 allocations: 15.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "madelon_svc_model = BaggingClassifier(LinearSVC(), n_jobs=-1)\n",
    "\n",
    "# Score the model\n",
    "madelon_svc_scores = average_cv_score(madelon_svc_model, madelon_train_features, madelon_train_labels)\n",
    "println(madelon_svc_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time madelon_svc_model.fit(madelon_train_features, madelon_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time madelon_svc_model.predict(madelon_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9881402451616971\n",
      "Training time:\n",
      " 21.900456 seconds (494.04 k allocations: 7.539 MiB)\n",
      "Testing time:\n",
      "  1.599752 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "kdd_svc_model = BaggingClassifier(LinearSVC(), n_jobs=-1)\n",
    "\n",
    "# Score the model\n",
    "kdd_svc_scores = average_cv_score(kdd_svc_model, kdd_train_features, kdd_train_labels)\n",
    "println(kdd_svc_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time kdd_svc_model.fit(kdd_train_features, kdd_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time kdd_svc_model.predict(kdd_test_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian (Gaussian) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7485682932684063\n",
      "Training time:\n",
      "  0.001121 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Testing time:\n",
      "  0.000412 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "car_nb_model = GaussianNB()\n",
    "\n",
    "# Score the model\n",
    "car_nb_scores = average_cv_score(car_nb_model, car_train_features, car_train_labels[:, 1])\n",
    "println(car_nb_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time car_nb_model.fit(car_train_features, car_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time car_nb_model.predict(car_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7379381805409203\n",
      "Training time:\n",
      "  0.012609 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Testing time:\n",
      "  0.005100 seconds (22.59 k allocations: 353.250 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "abalone_nb_model = GaussianNB()\n",
    "\n",
    "# Score the model\n",
    "abalone_nb_scores = average_cv_score(abalone_nb_model, abalone_train_features, abalone_train_labels[:, 1])\n",
    "println(abalone_nb_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time abalone_nb_model.fit(abalone_train_features, abalone_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time abalone_nb_model.predict(abalone_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5934615384615385\n",
      "Training time:\n",
      "  0.031000 seconds (17 allocations: 976 bytes)\n",
      "Testing time:\n",
      "  0.012870 seconds (33 allocations: 15.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "madelon_nb_model = GaussianNB()\n",
    "\n",
    "# Score the model\n",
    "madelon_nb_scores = average_cv_score(madelon_nb_model, madelon_train_features, madelon_train_labels[:, 1])\n",
    "println(madelon_nb_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time madelon_nb_model.fit(madelon_train_features, madelon_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time madelon_nb_model.predict(madelon_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9394580412234858\n",
      "Training time:\n",
      "  0.446924 seconds (494.04 k allocations: 7.539 MiB)\n",
      "Testing time:\n",
      "  0.112058 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "kdd_nb_model = GaussianNB()\n",
    "\n",
    "# Score the model\n",
    "kdd_nb_scores = average_cv_score(kdd_nb_model, kdd_train_features, kdd_train_labels[:, 1])\n",
    "println(kdd_nb_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time kdd_nb_model.fit(kdd_train_features, kdd_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time kdd_nb_model.predict(kdd_test_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net (MLP Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.645834 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Dict{Any,Any}(\"alpha\" => 0.001,\"learning_rate\" => \"invscaling\")\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid in a dictionary\n",
    "grid = Dict([\"alpha\" => exp10.(range(-5, -1, step=1)), \"learning_rate\" => [\"constant\", \"invscaling\", \"adaptive\"]])\n",
    "\n",
    "# Create the grid search class\n",
    "car_neural_search = GridSearchCV(estimator=MLPClassifier(), param_grid=grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# Time how long it takes to get the best parameters\n",
    "@time car_neural_search.fit(car_train_features, car_train_labels)\n",
    "println(car_neural_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8677205857137957\n",
      "Training time:\n",
      "  0.508937 seconds (1.22 k allocations: 19.562 KiB)\n",
      "Testing time:\n",
      "  0.000750 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters to a dictionary with Symbol keys\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in car_neural_search.best_params_)\n",
    "\n",
    "# Create the model using the best params\n",
    "car_neural_model = MLPClassifier(;best_params...)\n",
    "\n",
    "# Score the model\n",
    "car_neural_scores = average_cv_score(car_neural_model, car_train_features, car_train_labels)\n",
    "println(car_neural_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time car_neural_model.fit(car_train_features, car_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time car_neural_model.predict(car_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.685579 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Dict{Any,Any}(\"alpha\" => 0.001,\"learning_rate\" => \"adaptive\")\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid in a dictionary\n",
    "grid = Dict([\"alpha\" => exp10.(range(-5, -1, step=1)), \"learning_rate\" => [\"constant\", \"invscaling\", \"adaptive\"]])\n",
    "\n",
    "# Create the grid search class\n",
    "abalone_neural_search = GridSearchCV(estimator=MLPClassifier(), param_grid=grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# Time how long it takes to get the best parameters\n",
    "@time abalone_neural_search.fit(abalone_train_features, abalone_train_labels)\n",
    "println(abalone_neural_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7793332162510245\n",
      "Training time:\n",
      "  0.864102 seconds (58.47 k allocations: 913.859 KiB)\n",
      "Testing time:\n",
      "  0.005766 seconds (22.59 k allocations: 353.250 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters to a dictionary with Symbol keys\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in abalone_neural_search.best_params_)\n",
    "\n",
    "# Create the model using the best params\n",
    "abalone_neural_model = MLPClassifier(;best_params...)\n",
    "\n",
    "# Score the model\n",
    "abalone_neural_scores = average_cv_score(abalone_neural_model, abalone_train_features, abalone_train_labels)\n",
    "println(abalone_neural_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time abalone_neural_model.fit(abalone_train_features, abalone_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time abalone_neural_model.predict(abalone_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.493848 seconds (17 allocations: 976 bytes)\n",
      "Dict{Any,Any}(\"alpha\" => 9.999999999999999e-5,\"learning_rate\" => \"constant\")\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid in a dictionary\n",
    "grid = Dict([\"alpha\" => exp10.(range(-5, -1, step=1)), \"learning_rate\" => [\"constant\", \"invscaling\", \"adaptive\"]])\n",
    "\n",
    "# Create the grid search class\n",
    "madelon_neural_search = GridSearchCV(estimator=MLPClassifier(), param_grid=grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# Time how long it takes to get the best parameters\n",
    "@time madelon_neural_search.fit(madelon_train_features, madelon_train_labels)\n",
    "println(madelon_neural_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5034615384615384\n",
      "Training time:\n",
      "  1.171509 seconds (17 allocations: 976 bytes)\n",
      "Testing time:\n",
      "  0.011455 seconds (33 allocations: 15.828 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters to a dictionary with Symbol keys\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in madelon_neural_search.best_params_)\n",
    "\n",
    "# Create the model using the best params\n",
    "madelon_neural_model = MLPClassifier(;best_params...)\n",
    "\n",
    "# Score the model\n",
    "madelon_neural_scores = average_cv_score(madelon_neural_model, madelon_train_features, madelon_train_labels)\n",
    "println(madelon_neural_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time madelon_neural_model.fit(madelon_train_features, madelon_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time madelon_neural_model.predict(madelon_test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278.849733 seconds (494.04 k allocations: 7.539 MiB)\n",
      "Dict{Any,Any}(\"alpha\" => 1.0e-5,\"learning_rate\" => \"invscaling\")\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid in a dictionary\n",
    "grid = Dict([\"alpha\" => exp10.(range(-5, -1, step=1)), \"learning_rate\" => [\"constant\", \"invscaling\", \"adaptive\"]])\n",
    "\n",
    "# Create the grid search class\n",
    "kdd_neural_search = GridSearchCV(estimator=MLPClassifier(), param_grid=grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# Time how long it takes to get the best parameters\n",
    "@time kdd_neural_search.fit(kdd_train_features, kdd_train_labels)\n",
    "println(kdd_neural_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925914588649777\n",
      "Training time:\n",
      " 42.399160 seconds (494.04 k allocations: 7.539 MiB)\n",
      "Testing time:\n",
      "  0.379280 seconds (14 allocations: 800 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Convert the search's best parameters to a dictionary with Symbol keys\n",
    "best_params = Dict(Symbol(k) => v for (k, v) in kdd_neural_search.best_params_)\n",
    "\n",
    "# Create the model using the best params\n",
    "kdd_neural_model = MLPClassifier(;best_params...)\n",
    "\n",
    "# Score the model\n",
    "kdd_neural_scores = average_cv_score(kdd_neural_model, kdd_train_features, kdd_train_labels)\n",
    "println(kdd_neural_scores)\n",
    "\n",
    "# Time how long it takes to fit the model\n",
    "println(\"Training time:\")\n",
    "@time kdd_neural_model.fit(kdd_train_features, kdd_train_labels)\n",
    "\n",
    "# Time how long it takes to predict values\n",
    "println(\"Testing time:\")\n",
    "@time kdd_neural_model.predict(kdd_test_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_forest.model\t1703339 bytes\n",
      "abalone_forest.model\t6610940 bytes\n",
      "kdd_forest.model\t5097244 bytes\n",
      "madelon_forest.model\t4101990 bytes\n"
     ]
    }
   ],
   "source": [
    "forest_models = Dict([\"car_forest.model\" => car_forest_model, \"abalone_forest.model\" => abalone_forest_model, \"madelon_forest.model\" => madelon_forest_model, \"kdd_forest.model\" => kdd_forest_model])\n",
    "\n",
    "for (filename, model) in forest_models\n",
    "    joblib.dump(model, filename)\n",
    "    println(filename * \"\\t\" * string(stat(filename).size) * \" bytes\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madelon_svc.model\t85640 bytes\n",
      "abalone_svc.model\t6905 bytes\n",
      "kdd_svc.model\t12221 bytes\n",
      "car_svc.model\t6585 bytes\n"
     ]
    }
   ],
   "source": [
    "svc_models = Dict([\"car_svc.model\" => car_svc_model, \"abalone_svc.model\" => abalone_svc_model, \"madelon_svc.model\" => madelon_svc_model, \"kdd_svc.model\" => kdd_svc_model])\n",
    "\n",
    "for (filename, model) in svc_models\n",
    "    joblib.dump(model, filename)\n",
    "    println(filename * \"\\t\" * string(stat(filename).size) * \" bytes\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madelon_nb.model\t16696 bytes\n",
      "abalone_nb.model\t967 bytes\n",
      "car_nb.model\t903 bytes\n",
      "kdd_nb.model\t2055 bytes\n"
     ]
    }
   ],
   "source": [
    "nb_models = Dict([\"car_nb.model\" => car_nb_model, \"abalone_nb.model\" => abalone_nb_model, \"madelon_nb.model\" => madelon_nb_model, \"kdd_nb.model\" => kdd_nb_model])\n",
    "\n",
    "for (filename, model) in nb_models\n",
    "    joblib.dump(model, filename)\n",
    "    println(filename * \"\\t\" * string(stat(filename).size) * \" bytes\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone_neural.model\t39864 bytes\n",
      "car_neural.model\t34321 bytes\n",
      "kdd_neural.model\t143035 bytes\n",
      "madelon_neural.model\t1611979 bytes\n"
     ]
    }
   ],
   "source": [
    "neural_models = Dict([\"car_neural.model\" => car_neural_model, \"abalone_neural.model\" => abalone_neural_model, \"madelon_neural.model\" => madelon_neural_model, \"kdd_neural.model\" => kdd_neural_model])\n",
    "\n",
    "for (filename, model) in neural_models\n",
    "    joblib.dump(model, filename)\n",
    "    println(filename * \"\\t\" * string(stat(filename).size) * \" bytes\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
